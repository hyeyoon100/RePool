import torch
import json
from torch.nn.utils.rnn import pad_sequence

from token_rep import EntityTokenRep, RelationTokenRep
from entnode_prefilter import EntityNodeFilter
from data_preprocess import generate_token_labels
from utils import get_topk_candidates, build_bipartite_graph, TextEncoder
from triplet_scorer import BERTTripleScorer, extract_candidate_triples, node_id_to_token_map, convert_id_triples_to_text, split_triples_by_score
from llm_guidance import TripleSetEncoder,verbalize_triples, dpo_alignment_loss, ask_llm_preference


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# === ë°ì´í„° ë¡œë”© ===
with open("data/train_sample_scierc.json") as f:
        data = [json.loads(line) for line in f]

relation_types = ["used-for", "feature-of", "hyponym-of", "part-of", "compare", "conjunction", "evaluate-for"]


# === ë°ì´í„° ì¶”ì¶œ ===
sample = data[0]  # ì²« ë¬¸ì„œë§Œ í…ŒìŠ¤íŠ¸
sentences = sample["sentences"]
ner_spans = sample["ner"]

# input text ì„ë² ë”©
flattened_sentences = [" ".join(s) for s in sentences]
text_encoder = TextEncoder("bert-base-cased").to(device)
input_text_embedding = text_encoder(flattened_sentences)

# ê°œë³„ í† í°
entity_tokens = [tok for sent in sentences for tok in sent]  # flat list
# assert len(entity_tokens) == entity_emb.size(0)

# === ë¼ë²¨ ìƒì„± ===
labels = generate_token_labels(sentences, ner_spans)  # List[List[int]]
lengths = torch.tensor([len(s) for s in sentences])


# === ëª¨ë¸ êµ¬ì„± ===
token_model = EntityTokenRep(model_name="bert-base-cased")
relation_model = RelationTokenRep(num_relations=len(relation_types),
                                embedding_dim=768,
                                method="pretrained_lm",  # ë˜ëŠ” "onehot"
                                relation_names=relation_types
                            )
filter_model = EntityNodeFilter(hidden_size=768)

# === Token ì„ë² ë”© ===
out = token_model(sentences, lengths)
embeddings, mask = out["embeddings"], out["mask"]

flat_entity_emb = embeddings[mask.bool()]  # â†’ [N_entity, D]

# ë¼ë²¨ì„ í…ì„œë¡œ ë§Œë“¤ ë•Œ ë¬¸ì¥ ë³„ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”© ì²˜ë¦¬ 
label_tensor = pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=0)
flat_label_tensor = label_tensor[mask.bool()]  # [N_entity]


# === Relation node ì„ë² ë”© ===
rel_emb = relation_model()


# === Entity Node Pre-Filtering ì‹¤í–‰ ===
#! Loss
scores, prefilter_loss = filter_model(flat_entity_emb.unsqueeze(0), labels=flat_label_tensor.unsqueeze(0))

#~ candidate ì¶”ì¶œí•´ì„œ ê·¸ê±¸ë¡œ bipartite graph êµ¬ì„±í•  ë•Œ ì“´ ì½”ë“œ
# top_k = 7  # ë˜ëŠ” min(L, max_top_k) + add_top_k ê°™ì€ ë°©ì‹
# candidates: [B, L, D], label_tensor: [B, L], mask: [B, L], idx: [B, L]
# candidate_embeddings, candidate_labels, candidate_masks, candidate_indices = [
#     get_topk_candidates(scores, tensor, topk=top_k)[0] for tensor in [
#         embeddings, label_tensor, mask, torch.arange(embeddings.size(1)).unsqueeze(0).repeat(embeddings.size(0), 1)
#         # í•„í„°ë§ ìŠ¤ì½”ì–´(scores)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ top_kê°œì˜ í† í°ì„ ì—¬ëŸ¬ ê°œì˜ í…ì„œì—ì„œ ë™ì‹œì— ì¶”ì¶œí•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜
#         # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì˜ ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ì—¬ candidate_embeddings, candidate_labels, candidate_masks, candidate_indices ê°ê° ì €ì¥
#     ]
# ]

#~ original tokensì—ì„œ íƒˆë½í•œ ë…¸ë“œëŠ” keep_mask í• ë‹¹í•´ì„œ ì—£ì§€ ìƒì„± ì œí•œí•˜ëŠ” í˜•íƒœ
# scores: [B, L]
topk = 7
sorted_idx = torch.argsort(scores, dim=1, descending=True)
keep_mask = torch.zeros_like(scores)  # [B, L]
keep_mask.scatter_(1, sorted_idx[:, :topk], 1)  # ìƒìœ„ topkë§Œ 1ë¡œ ìœ ì§€


# === bidirectional complete bipartite graph ìƒì„± ===
# top-kë§Œ relationê³¼ edge ìƒì„±, íƒˆë½ëœ í† í°ì€ isolate nodeë¡œ ì¡´ì¬
# entity_emb = embeddings[0]
keep_ids = (keep_mask[0] == 1).nonzero(as_tuple=True)[0].tolist()
data = build_bipartite_graph(flat_entity_emb, keep_ids, rel_emb, bidirectional=True)

# Output:
# Data(x=[17, 768], edge_index=[2, 140], edge_type=[140], node_type=[17])

triples = extract_candidate_triples(data)
node_id_to_token = node_id_to_token_map(entity_tokens, relation_types)
text_triples = convert_id_triples_to_text(triples, node_id_to_token)


scorer = BERTTripleScorer("bert-base-cased")
token_level_scores = scorer(text_triples)

top_triples, top_scores, bottom_triples, bottom_scores = split_triples_by_score(
    text_triples,
    token_level_scores,
    top_k=10
)

print("ğŸ”¼ Top triples:")
for t, s in zip(top_triples, top_scores):
    print(f"{t} â†’ {s.item():.4f}")

print("ğŸ”½ Bottom triples:")
for t, s in zip(bottom_triples[:5], bottom_scores[:5]):  # 5ê°œë§Œ
    print(f"{t} â†’ {s.item():.4f}")


tripleset_encoder = TripleSetEncoder("bert-base-cased").to(device)

# top_triples, bottom_triples ì…ë ¥
top_vector = tripleset_encoder.encode_triple_set(top_triples)      # [H]
bottom_vector = tripleset_encoder.encode_triple_set(bottom_triples)  # [H]

# cosine similarity with input_text_embedding
cos = torch.nn.CosineSimilarity(dim=-1)
sim_top = cos(input_text_embedding, top_vector)
sim_btm = cos(input_text_embedding, bottom_vector)

print("Cosine similarity to input text:")
print("Top triple set â†’", sim_top.item())
print("Bottom triple set â†’", sim_btm.item())


# verbalize
summary_top = verbalize_triples(top_triples)
summary_btm = verbalize_triples(bottom_triples)

print("================")
print("summary_top: ", summary_top)
print("summary_btm: ", summary_btm)

# LLMìœ¼ë¡œ ì„ í˜¸ë„ í‰ê°€ ë°›ê¸°
preferred = ask_llm_preference(flattened_sentences, summary_top, summary_btm)

# DPO-style loss ê³„ì‚°
loss = dpo_alignment_loss(sim_top, sim_btm, preferred)