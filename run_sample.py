import torch
import json
from torch.nn.utils.rnn import pad_sequence
from collections import defaultdict

from token_rep import EntityTokenRep, RelationTokenRep
from prefilter import TokenFilter, SpanFilter
from generate_labels import generate_token_labels, generate_span_labels
from utils import build_bipartite_graph, TextEncoder, sample_triples, get_candidate_spans, get_span_embeddings, filter_candidate_spans, extract_token_relation_edges
from triplet_scorer import BERTTripleScorer, extract_candidate_triples, node_id_to_token_map, convert_id_triples_to_text, split_triples_by_score, extract_span_token_candidate_triples, remove_edges_of_bottom_triples
from llm_guidance import TripleSetEncoder,verbalize_triples, preference_learning_loss, ask_llm_preference
from aggregation import EntityEmbeddingUpdater, RelationEmbeddingUpdater

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# === ë°ì´í„° ë¡œë”© ===
with open("data/train_sample_scierc.json") as f:
        data = [json.loads(line) for line in f]

relation_types = ["used-for", "feature-of", "hyponym-of", "part-of", "compare", "conjunction", "evaluate-for"]
num_relations = len(relation_types)

# === ë°ì´í„° ì¶”ì¶œ ===
sample = data[0]  # ì²« ë¬¸ì„œë§Œ í…ŒìŠ¤íŠ¸
sentences = sample["sentences"]
ner_spans = sample["ner"]

# input text ì„ë² ë”©
# flattened_sentences = [" ".join(s) for s in sentences]
# input_text = " ".join([" ".join(sent) for sent in sentences])
flattened_sentences = " ".join(" ".join(sent) for sent in sentences)

# ì „ì²´ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©
text_encoder = TextEncoder("bert-base-cased").to(device)
input_text_embedding = text_encoder(flattened_sentences)

# ê°œë³„ í† í°
entity_tokens = [tok for sent in sentences for tok in sent]  # flat list
# assert len(entity_tokens) == entity_emb.size(0)

# === í† í° ë¼ë²¨ ìƒì„± ===
# (í•´ë‹¹ í† í°ì´ nerì— í¬í•¨ë˜ë©´ 1, í¬í•¨ ì•ˆë˜ë©´ 0) -> prefilterì˜ ì •ë‹µê°’ìœ¼ë¡œ ì‚¬ìš©í•¨
labels = generate_token_labels(sentences, ner_spans)  # List[List[int]]
lengths = torch.tensor([len(s) for s in sentences]).to(device)


# === ëª¨ë¸ êµ¬ì„± === (__init__)
token_model = EntityTokenRep(model_name="bert-base-cased").to(device)
relation_model = RelationTokenRep(
                    relation_types=relation_types,                     # relation ì´ë¦„ ë¦¬ìŠ¤íŠ¸
                    lm_model_name="bert-base-uncased",                 # ì‚¬ìš©í•  LM ì´ë¦„
                    freeze_bert=True                                   # ì„ë² ë”© ê³ ì • ì—¬ë¶€ (Trueë©´ í•™ìŠµ ì•ˆ í•¨)
                ).to(device)
token_filter = TokenFilter(hidden_size=768).to(device)
span_filter = SpanFilter(hidden_size=768).to(device)

#! hidden_size configì— ì •ì˜ëœ ê±° ì‚¬ìš©í•˜ê²Œ ë°”ê¾¸ê¸°?
# hidden_size = config.hidden_size  # ì¼ë°˜ì ìœ¼ë¡œ 768
# scorer = BERTTripleScorer(bert_model, hidden_size=hidden_size).to(device)
scorer = BERTTripleScorer(hidden_size=768).to(device)
tripleset_encoder = TripleSetEncoder().to(device)


# === Token ì„ë² ë”© ===
out = token_model(sentences, lengths)
embeddings, mask = out["embeddings"], out["mask"]

flat_entity_emb = embeddings[mask.bool()]  # [N, D]

# ë¼ë²¨ì„ í…ì„œë¡œ ë§Œë“¤ ë•Œ ë¬¸ì¥ ë³„ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”© ì²˜ë¦¬ 
label_tensor = pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=0).to(device)
flat_label_tensor = label_tensor[mask.bool()]  # [N_entity].to(device)


# === Relation node ì„ë² ë”© ===
rel_ids = torch.arange(num_relations).to(device)  # shape: [num_relations]
rel_emb = relation_model(rel_ids)


# === Entity Node Pre-Filtering ì‹¤í–‰ ===
#! Loss
scores, prefilter_loss = token_filter(flat_entity_emb.unsqueeze(0), labels=flat_label_tensor.unsqueeze(0))

#~ original tokensì—ì„œ íƒˆë½í•œ ë…¸ë“œëŠ” keep_mask í• ë‹¹í•´ì„œ ì—£ì§€ ìƒì„± ì œí•œí•˜ëŠ” í˜•íƒœ
# scores: [B, L]
topk = 12
sorted_idx = torch.argsort(scores, dim=1, descending=True)
keep_mask = torch.zeros_like(scores)  # [B, L]
keep_mask.scatter_(1, sorted_idx[:, :topk], 1)  # ìƒìœ„ topkë§Œ 1ë¡œ ìœ ì§€

# flat_keep_mask = keep_mask[mask.bool()]    # [N], 1ì´ë©´ keep, 0ì´ë©´ drop
# flat_keep_mask = keep_mask.view(-1)[mask.view(-1).bool()]
flat_keep_mask = keep_mask.view(-1)[:flat_entity_emb.size(0)]  # [N]


# Dropëœ í† í° ìœ„ì¹˜ë¥¼ ì œë¡œ ë²¡í„°ë¡œ masking
# ìŠ¤íŒ¬ ì„ë² ë”© ìƒì„± ì‹œ íƒˆë½ëœ í† í°ì´ í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•¨
flat_entity_emb = flat_entity_emb * flat_keep_mask.unsqueeze(-1)

all_node_emb = torch.cat([flat_entity_emb, rel_emb], dim=0)  # [N + R, D]

# === bidirectional complete bipartite graph ìƒì„± ===
# top-kë§Œ relationê³¼ edge ìƒì„±, íƒˆë½ëœ í† í°ì€ isolate nodeë¡œ ì¡´ì¬
# entity_emb = embeddings[0]
keep_ids = (keep_mask[0] == 1).nonzero(as_tuple=True)[0].tolist()
data = build_bipartite_graph(
                            all_node_emb, 
                            keep_ids, 
                            num_relation=rel_emb.size(0), 
                            bidirectional=True, 
                            keep_mask=flat_keep_mask.squeeze(0).tolist()
                            )


# Output:
# Data(x=[17, 768], edge_index=[2, 140], edge_type=[140], node_type=[17])

#* Token-level 
triples = extract_candidate_triples(data, compound_edge_type=0)
# node_id_to_token = node_id_to_token_map(entity_tokens, relation_types) # entity node, relation node id ê°’ ë¶€ì—¬
# text_triples = convert_id_triples_to_text(triples, node_id_to_token)

print('======triples ë¦¬ìŠ¤íŠ¸: ', triples[900:])

print('triples ê°œìˆ˜: ', len(triples))

# token_level_scores = scorer(text_triples)
# triple_cls_vectors = scorer.encode_triples(text_triples).to(device)  # [N, H] _ Tripleë³„ CLS ë²¡í„° ì¶”ì¶œ

#! triplesê°€ ì„ë² ë”©ìœ¼ë¡œ ë²„íŠ¸ì— ì…ë ¥ë˜ë„ë¡ ë°”ê¿”ì£¼ì–´ì•¼ í•¨. ì´ê±°ëŠ” ì•„ë§ˆ ìˆ«ì ê·¸ëŒ€ë¡œë¥¼ í…ìŠ¤íŠ¸ë¡œ í–ˆì„ ê²ƒ ê°™ìŒ.
token_level_scores = scorer(triples, all_node_emb)
triple_cls_vectors = scorer.encode_triples(triples, all_node_emb).to(device)  # [N, H] _ Tripleë³„ CLS ë²¡í„° ì¶”ì¶œ


top_triples, top_scores, bottom_triples, bottom_scores, top_indices, bottom_indices = split_triples_by_score(
    # text_triples,
    triples,
    token_level_scores,
    threshold=0.25,
    return_indices=True  # top/bottom tripleì˜ ì¸ë±ìŠ¤ë„ ë°˜í™˜í•˜ë„ë¡
)

# ì—£ì§€ ê°œìˆ˜: í•¨ìˆ˜ í˜¸ì¶œ ì „
num_edges_before = data.edge_index.size(1)
# num_edges_bf = data.edge_mask.sum().item()  # Trueì¸ ê°œìˆ˜ë§Œ ì¹´ìš´íŠ¸


# bottom íŠ¸ë¦¬í”Œ ì—£ì§€ ì‚­ì œ
data = remove_edges_of_bottom_triples(data, bottom_triples,
                                semantic_type_id=2,
                                compound_type_ids=(0, 1))

# ì—£ì§€ ê°œìˆ˜: í•¨ìˆ˜ í˜¸ì¶œ í›„
num_edges_after = data.edge_index.size(1)
# num_edges_af = data.edge_mask.sum().item()  # Trueì¸ ê°œìˆ˜ë§Œ ì¹´ìš´íŠ¸


print(f"# of edges before pruning: {num_edges_before}")
print(f"# of edges after pruning:  {num_edges_after}")
# print(f"# of edges_mask before pruning: {num_edges_bf}")
# print(f"# of edges_mask after pruning:  {num_edges_af}")


print("ğŸ”¼ Top triples:")
for t, s in zip(top_triples[:5], top_scores):
    print(f"{t} â†’ {s.item():.4f}")

print("ğŸ”½ Bottom triples:")
for t, s in zip(bottom_triples[:5], bottom_scores[:5]):  # 5ê°œë§Œ
    print(f"{t} â†’ {s.item():.4f}")

# #! top-k ì œì™¸ ë‚˜ë¨¸ì§€ ì—£ì§€ëŠ” ì‚­ì œí•´ì£¼ê¸°
# # ì €ì¥í•  edge ì •ë³´
# token_ids = list(range(len(flat_entity_emb)))  # token node ids
# keep_edges = set()

# for h, r, t in top_triples:
#     rel_node_id = r  # rel_idë¥¼ rel node idë¡œ ë³€í™˜
#     keep_edges.add((h, rel_node_id))  # entity â†’ relation
#     keep_edges.add((rel_node_id, t))  # relation â†’ entity

# # print(type(src), type(tgt))
# for e in keep_edges:
#     print(type(e[0]), type(e[1]))

# # edge filtering
# new_edge_index = []
# new_edge_type = []

# for i in range(data.edge_index.size(1)):
#     src = data.edge_index[0, i].item()
#     tgt = data.edge_index[1, i].item()
#     rel = data.edge_type[i].item()
#     print(f"CHECKING edge ({src}, {tgt}) â†’ keep? { (src, tgt) in keep_edges }")

#     if (src, tgt) in keep_edges:
#         new_edge_index.append([src, tgt])
#         new_edge_type.append(rel)

# print("new_edge_index): ", new_edge_index) 

# # ì¬êµ¬ì„±
# data.edge_index = torch.tensor(new_edge_index, dtype=torch.long).t().contiguous()
# data.edge_type = torch.tensor(new_edge_type, dtype=torch.long)

# print("======== ì¬êµ¬ì„±")
# print(data.edge_index)

# ê° top-k, bottom triplesì—ì„œ ì¼ë¶€ë§Œ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§
sample_size = 10  # ë‚˜ì¤‘ì— ë¹„ìœ¨ í˜•íƒœë¡œ ë³€ê²½

top_triples, top_scores, sampled_top_idx = sample_triples(top_triples, top_scores, sample_size, return_indices=True)
bottom_triples, bottom_scores, sampled_bottom_idx = sample_triples(bottom_triples, bottom_scores, sample_size, return_indices=True)

# === ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ë¥¼ ì›ë˜ triple_cls_vectorsì˜ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘ ===
# e.g., top_indices = [12, 35, 46, ...] â†’ sampled_top_idx = [1, 3, 5]
sampled_top_cls = triple_cls_vectors[[top_indices[i] for i in sampled_top_idx]]  # [sample_size, H]
sampled_bottom_cls = triple_cls_vectors[[bottom_indices[i] for i in sampled_bottom_idx]]  # [sample_size, H]


# ì´ì œ encode_triple_setì— ë„£ê¸°
top_vector = tripleset_encoder(sampled_top_cls)
bottom_vector = tripleset_encoder(sampled_bottom_cls)

# top_triples, bottom_triples ì…ë ¥
# top_vector = tripleset_encoder.encode_triple_set(top_triples)      # [H]
# bottom_vector = tripleset_encoder.encode_triple_set(bottom_triples)  # [H]

# cosine similarity with input_text_embedding
cos = torch.nn.CosineSimilarity(dim=-1)
sim_top = cos(input_text_embedding, top_vector).to(device)
sim_btm = cos(input_text_embedding, bottom_vector).to(device)

# print("Cosine similarity to input text:")
# print("Top triple set â†’", sim_top.item())
# print("Bottom triple set â†’", sim_btm.item())


# verbalize _ ë‹¨ìˆœ ì´ì–´ë¶™ì´ëŠ” í˜•íƒœ?
summary_top = verbalize_triples(top_triples)
summary_btm = verbalize_triples(bottom_triples)

# print("================")
# print("summary_top: ", summary_top)
# print("summary_btm: ", summary_btm)

# LLMìœ¼ë¡œ ì„ í˜¸ë„ í‰ê°€ ë°›ê¸°
# "A" ë˜ëŠ” "B"
preferred = ask_llm_preference(flattened_sentences, summary_top, summary_btm)

# DPO-style loss ê³„ì‚°
#! token-lvel preference learning loss <- ìˆ˜ì • í•„ìš”í•¨
tok_pf_loss = preference_learning_loss(sim_top, sim_btm, preferred)


#! Validity-Aware Aggregation
# 1. top tripleë“¤ì˜ head id ì¶”ì¶œ
top_head_ids = [triples[idx][0] for idx in top_indices]  # h from (h, r, t, type)

# 2. ì¤‘ë³µ ì œê±°
unique_head_ids = set([triples[i][0] for i in top_indices])

# 3. ì„ë² ë”© ë³µì‚¬ë³¸ ìƒì„± (ì—…ë°ì´íŠ¸ë  ê²ƒ)
update_entity_embedding = EntityEmbeddingUpdater(hidden_dim=flat_entity_emb.size(1)).to(device)
updated_entity_emb = flat_entity_emb.clone()

top_cls_vectors = triple_cls_vectors[top_indices] 

# 4. ê° head_idì— ëŒ€í•´ ì—…ë°ì´íŠ¸ ì ìš©
#! ì¼ë‹¨ ì§€ê¸ˆì€ weighted sumì—ì„œ weightë¥¼ ìœ ë‹ˆí¼í•˜ê²Œ 1ë¡œ ì£¼ëŠ”ë°, ì´ê±°ë¥¼ score ê¸°ë°˜ìœ¼ë¡œ weight í• ë‹¹?
for head_id in unique_head_ids:
    updated_entity_emb[head_id] = update_entity_embedding(
        entity_emb=flat_entity_emb,
        cls_embeddings=top_cls_vectors,     # shape: [K, D]
        triple_indices=top_indices,            # ê¸¸ì´ K
        triples=triples,                       # full triple list
        target_head_id=head_id
    )

update_relation_embedding = RelationEmbeddingUpdater(hidden_dim=rel_emb.size(1)).to(device)

# ì „ì²´ ë…¸ë“œ ì„ë² ë”©: [num_entity + num_relation, D]
updated_relation_embedding = update_relation_embedding(rel_emb, node_type)




#* Span-level

# # span-level ë…¸ë“œ ìƒì„± (connected ì—£ì§€ ê¸°ì¤€, window size ì§€ì •, ê°œì¸ë…¸ë“œ-ìŠ¤íŒ¬ë…¸ë“œ )
# spans = generate_span_candidates(sentences)
# print('spans: ', spans)

# # span pre-filter
# span_labels = 


#* token-to-span composition layer
# Candidate Span ìƒì„±
# ë¬¸ì¥ë³„ë¡œ window size ì´ë‚´ì˜ ëª¨ë“  (start, end) ìƒì„±
keep_mask_list = flat_keep_mask.view(-1).tolist()
candidate_spans = get_candidate_spans(sentences, window_size=5, keep_mask=keep_mask_list)

# ì •ë‹µ ë¼ë²¨ (nerì— ìˆìœ¼ë©´ ì •ë‹µ 1, ì—†ìœ¼ë©´ 0)
span_labels = generate_span_labels(candidate_spans, ner_spans)  # [N_spans]

print('candidate_spans: ', candidate_spans[:20])
print('span_labels: ', span_labels[:10])

# ìŠ¤íŒ¬ ì„ë² ë”© ìƒì„± (ë‚´ë¶€ í† í°ì€ ì¼ë‹¨ mean poolingìœ¼ë¡œ êµ¬í˜„)
span_embs = get_span_embeddings(flat_entity_emb, candidate_spans)  # [N_spans, D]
span_labels_tensor = torch.tensor(span_labels).unsqueeze(0).to(device)
# span_ids = list(range(len(candidate_spans)))

# Span Node Prefilter
#! span prefilter Loss
span_scores, span_loss = span_filter(span_embs.unsqueeze(0), labels=span_labels_tensor)
candidate_spans, topk_indices = filter_candidate_spans(span_scores, candidate_spans, topk_ratio=0.4)


# === Node Index ì¬ì •ì˜ (bipartite êµ¬ì¡° _ token node -> relation node -> span node ì¸ë±ìŠ¤ ìˆœì„œ)
token_ids = list(range(len(entity_tokens)))  # token node ids
relation_ids = list(range(len(token_ids), len(token_ids) + len(relation_types)))
span_ids = list(range(len(token_ids) + len(relation_types), len(token_ids) + len(relation_types) + len(candidate_spans)))

# === span_id -> í¬í•¨ëœ token_id ë§¤í•‘
# {155: [110, 111]} : 155ë²ˆ ë…¸ë“œ(ìŠ¤íŒ¬)ì€ í† í° 110, 111ë¡œ êµ¬ì„±ë˜ì–´ìˆìŒì„ ì˜ë¯¸
span_id_to_token_indices = {
    span_ids[i]: list(range(start, end + 1))
    for i, (start, end) in enumerate(candidate_spans)
}
print("span_id_to_token_indices:", span_id_to_token_indices)


# === ìŠ¤íŒ¬ ë…¸ë“œì˜ relation ì—£ì§€ ì—°ê²° (ë‚´ë¶€ í† í°ì´ ì—°ê²°ëœ relë§Œ)
span_rel_edges = []  # (span_id, rel_id, rel_node_id)
token_rel_edges = extract_token_relation_edges(data)  #! (token_id, rel_id, rel_node_id)

for span_id, token_list in span_id_to_token_indices.items():
    for token_id in token_list:
        for t_id, rel_id, rel_node_id in token_rel_edges:
            if token_id == t_id:
                span_rel_edges.append((span_id, rel_id, rel_node_id))

# ì¤‘ë³µ ë°©ì§€ _
# span ë‚´ë¶€ì— ë™ì¼í•œ relationì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•  ê²½ìš° ì¤‘ë³µëœ (span_id, rel_id, rel_node_id)ê°€ ì—¬ëŸ¬ ë²ˆ appendë˜ê¸° ë•Œë¬¸
span_rel_edges = list(set(span_rel_edges))

# === ì‚¬ì „ ì²˜ë¦¬: span_id â†’ rel_id mapping ë§Œë“¤ê¸°
# ìŠ¤íŒ¬ ë…¸ë“œê°€ ì–´ë–¤ relation íƒ€ì…ë“¤ê³¼ ì—°ê²°ë  ìˆ˜ ìˆëŠ”ì§€
span_relation_candidates = defaultdict(set)
for span_id, rel_id, rel_node_id in span_rel_edges:
    span_relation_candidates[span_id].add(rel_id)

print("span_relation_candidates: ", span_relation_candidates)

# === candidate triple ìƒì„± (token-span, span-span)
total_node_ids = token_ids + span_ids  # í˜„ì¬ëŠ” relation nodeëŠ” ì œì™¸
total_span_ids = span_ids

# span_id_to_token_indices ëŠ” ë¦¬í„´ë°›ì„ í•„ìš” X?
triples = extract_span_token_candidate_triples(
    token_ids=token_ids,
    span_ids=span_ids,
    span_relation_candidates=span_relation_candidates,
    span_id_to_token_indices=span_id_to_token_indices,
    relation_types=relation_types
)

print('span_triples: ', triples)










# # í•„í„°ë§ ëœ ìŠ¤íŒ¬ë§Œ ê°€ì§€ê³  ì¸ë±ìŠ¤ ë¶€ì—¬ (-> íŠ¸ë¦¬í”Œ ì¡°í•© ìƒì„± ì‹œ ì¸ë±ìŠ¤ í•„ìš”)
# span_labels = [span_labels[i] for i in topk_indices]
# # span_ids = [span_ids[i] for i in topk_indices]
# span_embs = span_embs.squeeze(0)[topk_indices]  # [N, D]

# #! ìŠ¤íŒ¬ ë…¸ë“œ ìƒì„±í•˜ì—¬ ê·¸ë˜í”„ì— ì¶”ê°€í•´ì£¼ëŠ” ê³¼ì • ì¶”ê°€



# # ===ì—¬ê¸°ë¶€í„°ëŠ” token-levelê³¼ ë™ì¼í•œ í”Œë¡œìš°

# token_ids = list(range(len(flat_entity_emb)))  
# # span node IDëŠ” token ë…¸ë“œ ID ë‹¤ìŒë¶€í„° _ ìŠ¤íŒ¬ë„ ë…¸ë“œë¡œ 
# span_ids = list(range(len(token_ids), len(token_ids) + len(candidate_spans)))

# # ìŠ¤íŒ¬ì„ êµ¬ì„±í•˜ê³  ìˆëŠ” í† í°ì˜ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë‹´ìŒ
# span_id_to_token_indices = {
#     span_ids[i]: list(range(start, end + 1))
#     for i, (start, end) in enumerate(candidate_spans)
# }

# print("span_id_to_token_indices: ", span_id_to_token_indices)

# #! ì •ì˜í•´ì¤˜ì•¼í•¨ _ token levelì—ì„œ ì—£ì§€ í’€ë§ ì •ë³´ ê°±ì‹ í•˜ëŠ” ê²ƒë„.. 
# token_rel_edges = extract_token_relation_edges(data)  # ë˜ëŠ” ì €ì¥ëœ token-level ì—£ì§€

# triples, span_id_to_token_indices = extract_span_token_candidate_triples_topk_pooling(
#     token_ids=token_ids,
#     span_ids=span_ids,
#     candidate_spans=candidate_spans,
#     span_id_to_token_indices=span_id_to_token_indices,
#     token_rel_edges=token_rel_edges,
#     relation_types=relation_types
# )

# token_level_scores = scorer(text_triples)
# triple_cls_vectors = scorer.encode_triples(text_triples).to(device)  # [N, H] _ Tripleë³„ CLS ë²¡í„° ì¶”ì¶œ

# top_triples, top_scores, bottom_triples, bottom_scores, top_indices, bottom_indices = split_triples_by_score(
#     text_triples,
#     token_level_scores,
#     top_k=13,
#     return_indices=True  # top/bottom tripleì˜ ì¸ë±ìŠ¤ë„ ë°˜í™˜í•˜ë„ë¡
# )


# # === ì„ë² ë”© í†µí•©
# node_embs = torch.cat([flat_entity_emb, span_embs], dim=0)

# # === triple CLS ì„ë² ë”© ì¶”ì¶œ
# triple_cls_vectors = scorer.encode_triples(
#     span_triples, node_embs, relation_emb
# ).to(device)

# # === triple scoring
# span_level_scores = scorer.score_triple_vectors(triple_cls_vectors)

# # === top-k/bottom-k ë¶„ë¦¬
# top_triples, top_scores, bottom_triples, bottom_scores, top_indices, bottom_indices = split_triples_by_score(
#     span_triples,
#     span_level_scores,
#     top_k=13,
#     return_indices=True
# )
